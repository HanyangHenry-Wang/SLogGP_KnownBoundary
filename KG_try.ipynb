{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanyang/anaconda3/envs/known_boundary/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC\n",
    "from copy import deepcopy\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition.acquisition import AcquisitionFunction\n",
    "#from botorch.acquisition.objective import ScalarizedObjective\n",
    "from botorch.exceptions import UnsupportedError\n",
    "from botorch.models.gp_regression import FixedNoiseGP\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.models.model import Model\n",
    "from botorch.posteriors.posterior import Posterior\n",
    "#from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.utils.transforms import convert_to_target_pre_hook, t_batch_mode_transform\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition.acquisition import AcquisitionFunction\n",
    "from botorch.acquisition.objective import PosteriorTransform\n",
    "from botorch.exceptions import UnsupportedError\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.models.model import Model\n",
    "from botorch.utils.constants import get_constants_like\n",
    "from botorch.utils.probability import MVNXPB\n",
    "from botorch.utils.probability.utils import (\n",
    "    log_ndtr as log_Phi,\n",
    "    log_phi,\n",
    "    log_prob_normal_in,\n",
    "    ndtr as Phi,\n",
    "    phi,\n",
    ")\n",
    "from botorch.utils.safe_math import log1mexp, logmeanexp\n",
    "from botorch.utils.transforms import convert_to_target_pre_hook, t_batch_mode_transform\n",
    "from gpytorch.likelihoods.gaussian_likelihood import FixedNoiseGaussianLikelihood\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "# the following two numbers are needed for _log_ei_helper\n",
    "_neg_inv_sqrt2 = -(2**-0.5)\n",
    "_log_sqrt_pi_div_2 = math.log(math.pi / 2) / 2\n",
    "\n",
    "\n",
    "class AnalyticAcquisitionFunction(AcquisitionFunction, ABC):\n",
    "    r\"\"\"\n",
    "    Base class for analytic acquisition functions.\n",
    "\n",
    "    :meta private:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"Base constructor for analytic acquisition functions.\n",
    "\n",
    "        Args:\n",
    "            model: A fitted single-outcome model.\n",
    "            posterior_transform: A PosteriorTransform. If using a multi-output model,\n",
    "                a PosteriorTransform that transforms the multi-output posterior into a\n",
    "                single-output posterior is required.\n",
    "        \"\"\"\n",
    "        super().__init__(model=model)\n",
    "        if posterior_transform is None:\n",
    "            if model.num_outputs != 1:\n",
    "                raise UnsupportedError(\n",
    "                    \"Must specify a posterior transform when using a \"\n",
    "                    \"multi-output model.\"\n",
    "                )\n",
    "        else:\n",
    "            if not isinstance(posterior_transform, PosteriorTransform):\n",
    "                raise UnsupportedError(\n",
    "                    \"AnalyticAcquisitionFunctions only support PosteriorTransforms.\"\n",
    "                )\n",
    "        self.posterior_transform = posterior_transform\n",
    "\n",
    "    def set_X_pending(self, X_pending: Optional[Tensor] = None) -> None:\n",
    "        raise UnsupportedError(\n",
    "            \"Analytic acquisition functions do not account for X_pending yet.\"\n",
    "        )\n",
    "\n",
    "    def _mean_and_sigma(\n",
    "        self, X: Tensor, compute_sigma: bool = True, min_var: float = 1e-12\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"Computes the first and second moments of the model posterior.\n",
    "\n",
    "        Args:\n",
    "            X: `batch_shape x q x d`-dim Tensor of model inputs.\n",
    "            compute_sigma: Boolean indicating whether or not to compute the second\n",
    "                moment (default: True).\n",
    "            min_var: The minimum value the variance is clamped too. Should be positive.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of tensors containing the first and second moments of the model\n",
    "            posterior. Removes the last two dimensions if they have size one. Only\n",
    "            returns a single tensor of means if compute_sigma is True.\n",
    "        \"\"\"\n",
    "        self.to(device=X.device)  # ensures buffers / parameters are on the same device\n",
    "        posterior = self.model.posterior(\n",
    "            X=X, posterior_transform=self.posterior_transform\n",
    "        )\n",
    "        mean = posterior.mean.squeeze(-2).squeeze(-1)  # removing redundant dimensions\n",
    "        if not compute_sigma:\n",
    "            return mean, None\n",
    "        sigma = posterior.variance.clamp_min(min_var).sqrt().view(mean.shape)\n",
    "        return mean, sigma\n",
    "\n",
    "class DiscreteKnowledgeGradient(AnalyticAcquisitionFunction):\n",
    "    r\"\"\"Knowledge Gradient using a fixed discretisation in the Design Space \"X\".\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        bounds: Optional[Tensor] = None,\n",
    "        num_discrete_points: Optional[int] = None,\n",
    "        X_discretisation: Optional[Tensor] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Discrete Knowledge Gradient\n",
    "        Args:\n",
    "            model: A fitted model.\n",
    "            bounds: A `2 x d` tensor of lower and upper bounds for each column\n",
    "            num_discrete_points: (int) The number of discrete points to use for input (X) space. More discrete\n",
    "                points result in a better approximation, at the expense of\n",
    "                memory and wall time.\n",
    "            discretisation: A `k x d`-dim Tensor of `k` design points that will approximate the\n",
    "                continuous space with a discretisation.\n",
    "        \"\"\"\n",
    "\n",
    "        if X_discretisation is None:\n",
    "            if num_discrete_points is None:\n",
    "                raise ValueError(\n",
    "                    \"Must specify `num_discrete_points` for random discretisation if no `discretisation` is provided.\"\n",
    "                )\n",
    "\n",
    "            X_discretisation = draw_sobol_samples(\n",
    "                bounds=bounds, n=num_discrete_points, q=1\n",
    "            )\n",
    "\n",
    "        super(AnalyticAcquisitionFunction, self).__init__(model=model)\n",
    "\n",
    "        self.num_input_dimensions = bounds.shape[1]\n",
    "        self.X_discretisation = X_discretisation\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1, assert_output_shape=False)\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        kgvals = torch.zeros(X.shape[0], dtype=torch.double)\n",
    "        for xnew_idx, xnew in enumerate(X):\n",
    "            xnew = xnew.unsqueeze(0)\n",
    "            kgvals[xnew_idx] = self.compute_discrete_kg(\n",
    "                xnew=xnew, optimal_discretisation=self.X_discretisation\n",
    "            )\n",
    "        return kgvals\n",
    "\n",
    "    def compute_discrete_kg(\n",
    "        self, xnew: Tensor, optimal_discretisation: Tensor\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "        xnew: A `1 x 1 x d` Tensor with `1` acquisition function evaluations of\n",
    "            `d` dimensions.\n",
    "            optimal_discretisation: num_fantasies x d Tensor. Optimal X values for each z in zvalues.\n",
    "\n",
    "        \"\"\"\n",
    "        # Augment the discretisation with the designs.\n",
    "        concatenated_xnew_discretisation = torch.cat(\n",
    "            [xnew, optimal_discretisation], dim=0\n",
    "        ).squeeze()  # (m + num_X_disc, d)\n",
    "\n",
    "        # Compute posterior mean, variance, and covariance.\n",
    "        full_posterior = self.model.posterior(\n",
    "            concatenated_xnew_discretisation, observation_noise=False\n",
    "        )\n",
    "        noise_variance = torch.unique(self.model.likelihood.noise_covar.noise)\n",
    "        full_posterior_mean = full_posterior.mean  # (1 + num_X_disc , 1)\n",
    "\n",
    "        # Compute full Covariante Cov(Xnew, X_discretised), select [Xnew X_discretised] submatrix, and subvectors.\n",
    "        full_posterior_covariance = (\n",
    "            full_posterior.mvn.covariance_matrix\n",
    "        )  # (1 + num_X_disc , 1 + num_X_disc )\n",
    "        posterior_cov_xnew_opt_disc = full_posterior_covariance[\n",
    "            : len(xnew), :\n",
    "        ].squeeze()  # ( 1 + num_X_disc,)\n",
    "        full_posterior_variance = (\n",
    "            full_posterior.variance.squeeze()\n",
    "        )  # (1 + num_X_disc, )\n",
    "\n",
    "        full_predictive_covariance = (\n",
    "            posterior_cov_xnew_opt_disc\n",
    "            / (full_posterior_variance + noise_variance).sqrt()\n",
    "        )\n",
    "        # initialise empty kgvals torch.tensor\n",
    "        kgval = self.kgcb(a=full_posterior_mean, b=full_predictive_covariance)\n",
    "\n",
    "        return kgval\n",
    "\n",
    "    @staticmethod\n",
    "    def kgcb(a: Tensor, b: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Calculates the linear epigraph, i.e. the boundary of the set of points\n",
    "        in 2D lying above a collection of straight lines y=a+bx.\n",
    "        Parameters\n",
    "        ----------\n",
    "        a\n",
    "            Vector of intercepts describing a set of straight lines\n",
    "        b\n",
    "            Vector of slopes describing a set of straight lines\n",
    "        Returns\n",
    "        -------\n",
    "        KGCB\n",
    "            average height of the epigraph\n",
    "        \"\"\"\n",
    "\n",
    "        a = a.squeeze()\n",
    "        b = b.squeeze()\n",
    "        assert len(a) > 0, \"must provide slopes\"\n",
    "        assert len(a) == len(b), f\"#intercepts != #slopes, {len(a)}, {len(b)}\"\n",
    "\n",
    "        maxa = torch.max(a)\n",
    "\n",
    "        if torch.all(torch.abs(b) < 0.000000001):\n",
    "            return torch.Tensor([0])  # , np.zeros(a.shape), np.zeros(b.shape)\n",
    "\n",
    "        # Order by ascending b and descending a. There should be an easier way to do this\n",
    "        # but it seems that pytorch sorts everything as a 1D Tensor\n",
    "\n",
    "        ab_tensor = torch.vstack([-a, b]).T\n",
    "        ab_tensor_sort_a = ab_tensor[ab_tensor[:, 0].sort()[1]]\n",
    "        ab_tensor_sort_b = ab_tensor_sort_a[ab_tensor_sort_a[:, 1].sort()[1]]\n",
    "        a = -ab_tensor_sort_b[:, 0]\n",
    "        b = ab_tensor_sort_b[:, 1]\n",
    "\n",
    "        # exclude duplicated b (or super duper similar b)\n",
    "        threshold = (b[-1] - b[0]) * 0.00001\n",
    "        diff_b = b[1:] - b[:-1]\n",
    "        keep = diff_b > threshold\n",
    "        keep = torch.cat([torch.Tensor([True]), keep])\n",
    "        keep[torch.argmax(a)] = True\n",
    "        keep = keep.bool()  # making sure 0 1's are transformed to booleans\n",
    "\n",
    "        a = a[keep]\n",
    "        b = b[keep]\n",
    "\n",
    "        # initialize\n",
    "        idz = [0]\n",
    "        i_last = 0\n",
    "        x = [-torch.inf]\n",
    "\n",
    "        n_lines = len(a)\n",
    "        # main loop TODO describe logic\n",
    "        # TODO not pruning properly, e.g. a=[0,1,2], b=[-1,0,1]\n",
    "        # returns x=[-inf, -1, -1, inf], shouldn't affect kgcb\n",
    "        while i_last < n_lines - 1:\n",
    "            i_mask = torch.arange(i_last + 1, n_lines)\n",
    "            x_mask = -(a[i_last] - a[i_mask]) / (b[i_last] - b[i_mask])\n",
    "\n",
    "            best_pos = torch.argmin(x_mask)\n",
    "            idz.append(i_mask[best_pos])\n",
    "            x.append(x_mask[best_pos])\n",
    "\n",
    "            i_last = idz[-1]\n",
    "\n",
    "        x.append(torch.inf)\n",
    "\n",
    "        x = torch.Tensor(x)\n",
    "        idz = torch.LongTensor(idz)\n",
    "        # found the epigraph, now compute the expectation\n",
    "        a = a[idz]\n",
    "        b = b[idz]\n",
    "\n",
    "        normal = Normal(torch.zeros_like(x), torch.ones_like(x))\n",
    "\n",
    "        pdf = torch.exp(normal.log_prob(x))\n",
    "        cdf = normal.cdf(x)\n",
    "\n",
    "        kg = torch.sum(a * (cdf[1:] - cdf[:-1]) + b * (pdf[:-1] - pdf[1:]))\n",
    "        kg -= maxa\n",
    "        return kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botorch\n",
    "from known_boundary.acquisition_function import EI_acquisition_opt,MES_acquisition_opt,LCB_acquisition_opt,ERM_acquisition_opt,SLogTEI_acquisition_opt,SLogEI_acquisition_opt\n",
    "from known_boundary.utlis import  get_initial_points,transform,opt_model_MLE,opt_model_MAP\n",
    "import numpy as np\n",
    "import GPy\n",
    "import torch\n",
    "from botorch.test_functions import Ackley,Beale,Branin,Rosenbrock,SixHumpCamel,Hartmann,Powell,DixonPrice,Levy,StyblinskiTang,Griewank\n",
    "import obj_functions.push_problems\n",
    "from botorch.utils.transforms import unnormalize,normalize\n",
    "from known_boundary.SLogGP import SLogGP\n",
    "import scipy \n",
    "\n",
    "\n",
    "from botorch.models import SingleTaskGP,FixedNoiseGP\n",
    "from botorch.acquisition import ExpectedImprovement,PosteriorMean,qKnowledgeGradient\n",
    "from botorch.optim import optimize_acqf\n",
    "from torch.quasirandom import SobolEngine\n",
    "from gpytorch.kernels import MaternKernel, RBFKernel, IndexKernel\n",
    "from gpytorch.kernels.scale_kernel import ScaleKernel\n",
    "from gpytorch.means import ZeroMean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger('lengthscale').disabled = True\n",
    "logging.getLogger('variance').disabled = True\n",
    "logging.getLogger('psi').disabled = True\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "\n",
    "\n",
    "function_information = []\n",
    "\n",
    "\n",
    "temp={}\n",
    "temp['name']='Branin2D' \n",
    "temp['function'] = Branin(negate=True)\n",
    "temp['fstar'] =  0.397887 \n",
    "function_information.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fstar is:  0.397887\n",
      "Branin2D\n",
      "0\n",
      "0\n",
      "lengthscale:  0.25472596777010337\n",
      "variance:  0.9502835652673391\n",
      "-3.5451968551073154\n",
      "noise:  1e-05\n",
      "1\n",
      "-3.5451968551073154\n",
      "noise:  1e-05\n",
      "2\n",
      "lengthscale:  0.26427288507641644\n",
      "variance:  0.8397149780705103\n",
      "-1.9857158270411155\n",
      "noise:  8e-06\n",
      "3\n",
      "-1.9857158270411155\n",
      "noise:  8e-06\n",
      "4\n",
      "lengthscale:  0.2873438169759484\n",
      "variance:  1.115755989042731\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "5\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "6\n",
      "lengthscale:  0.3451401182139733\n",
      "variance:  1.359889269642337\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "7\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "8\n",
      "lengthscale:  0.34948060373411033\n",
      "variance:  2.0152997900044616\n",
      "-1.9857158270411155\n",
      "noise:  2e-05\n",
      "9\n",
      "-1.9857158270411155\n",
      "noise:  2e-05\n",
      "10\n",
      "lengthscale:  0.38904901362863364\n",
      "variance:  2.793129397859841\n",
      "-1.9857158270411155\n",
      "noise:  3e-05\n",
      "11\n",
      "-1.9857158270411155\n",
      "noise:  3e-05\n",
      "12\n",
      "lengthscale:  0.288197487780583\n",
      "variance:  1.4219478171201851\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "13\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "14\n",
      "lengthscale:  0.26034991859642936\n",
      "variance:  1.3497957921600225\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "15\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "16\n",
      "lengthscale:  0.23983785763856624\n",
      "variance:  1.2540111240535703\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "17\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "18\n",
      "lengthscale:  0.2233913739092695\n",
      "variance:  1.2607883466254137\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "19\n",
      "-1.9857158270411155\n",
      "noise:  1e-05\n",
      "20\n",
      "lengthscale:  0.23650433718091274\n",
      "variance:  1.57157201197233\n",
      "-1.9857158270411155\n",
      "noise:  2e-05\n",
      "21\n",
      "-1.9857158270411155\n",
      "noise:  2e-05\n",
      "22\n",
      "KeyboardInterrupt caught, calling on_optimization_end() to round things up\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m# train the GP\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39mstep_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     parameters \u001b[39m=\u001b[39m opt_model_MLE(train_X,train_Y,dim,\u001b[39m'\u001b[39;49m\u001b[39mGP\u001b[39;49m\u001b[39m'\u001b[39;49m,noise\u001b[39m=\u001b[39;49mnoise,seed\u001b[39m=\u001b[39;49mi,lengthscale_range\u001b[39m=\u001b[39;49mlengthscale_range,variance_range\u001b[39m=\u001b[39;49mvariance_range)\n\u001b[1;32m     74\u001b[0m     lengthscale \u001b[39m=\u001b[39m parameters[\u001b[39m0\u001b[39m]\n\u001b[1;32m     75\u001b[0m     variance \u001b[39m=\u001b[39m parameters[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Project/SLogGP_KnownBoundary/known_boundary/utlis.py:80\u001b[0m, in \u001b[0;36mopt_model_MLE\u001b[0;34m(train_X, train_Y, dim, model_type, noise, seed, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(ii\u001b[39m+\u001b[39mseed)\n\u001b[1;32m     78\u001b[0m random\u001b[39m.\u001b[39mseed(ii\u001b[39m+\u001b[39mseed)\n\u001b[0;32m---> 80\u001b[0m m\u001b[39m.\u001b[39;49moptimize()\n\u001b[1;32m     82\u001b[0m obj_temp \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mm\u001b[39m.\u001b[39mlog_likelihood()\n\u001b[1;32m     83\u001b[0m lengthscale_temp \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mrbf\u001b[39m.\u001b[39mlengthscale\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/core/gp.py:675\u001b[0m, in \u001b[0;36mGP.optimize\u001b[0;34m(self, optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minference_method\u001b[39m.\u001b[39mon_optimization_start()\n\u001b[1;32m    674\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 675\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(GP, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49moptimize(optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    676\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    677\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mKeyboardInterrupt caught, calling on_optimization_end() to round things up\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/model.py:111\u001b[0m, in \u001b[0;36mModel.optimize\u001b[0;34m(self, optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     opt \u001b[39m=\u001b[39m optimizer(max_iters\u001b[39m=\u001b[39mmax_iters, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    110\u001b[0m \u001b[39mwith\u001b[39;00m VerboseOptimization(\u001b[39mself\u001b[39m, opt, maxiters\u001b[39m=\u001b[39mmax_iters, verbose\u001b[39m=\u001b[39mmessages, ipython_notebook\u001b[39m=\u001b[39mipython_notebook, clear_after_finish\u001b[39m=\u001b[39mclear_after_finish) \u001b[39mas\u001b[39;00m vo:\n\u001b[0;32m--> 111\u001b[0m     opt\u001b[39m.\u001b[39;49mrun(start, f_fp\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_objective_grads, f\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_objective, fp\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grads)\n\u001b[1;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_array \u001b[39m=\u001b[39m opt\u001b[39m.\u001b[39mx_opt\n\u001b[1;32m    115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimization_runs\u001b[39m.\u001b[39mappend(opt)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/optimization/optimization.py:51\u001b[0m, in \u001b[0;36mOptimizer.run\u001b[0;34m(self, x_init, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, x_init, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     50\u001b[0m     start \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt(x_init, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     52\u001b[0m     end \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m     53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(end \u001b[39m-\u001b[39m start)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/optimization/optimization.py:124\u001b[0m, in \u001b[0;36mopt_lbfgsb.opt\u001b[0;34m(self, x_init, f_fp, f, fp)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbfgs_factor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     opt_dict[\u001b[39m'\u001b[39m\u001b[39mfactr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbfgs_factor\n\u001b[0;32m--> 124\u001b[0m opt_result \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mfmin_l_bfgs_b(f_fp, x_init, maxfun\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iters, maxiter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iters, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopt_dict)\n\u001b[1;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_opt \u001b[39m=\u001b[39m opt_result[\u001b[39m0\u001b[39m]\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_opt \u001b[39m=\u001b[39m f_fp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_opt)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:197\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39m# build options\u001b[39;00m\n\u001b[1;32m    186\u001b[0m opts \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: disp,\n\u001b[1;32m    187\u001b[0m         \u001b[39m'\u001b[39m\u001b[39miprint\u001b[39m\u001b[39m'\u001b[39m: iprint,\n\u001b[1;32m    188\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaxcor\u001b[39m\u001b[39m'\u001b[39m: m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcallback\u001b[39m\u001b[39m'\u001b[39m: callback,\n\u001b[1;32m    195\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaxls\u001b[39m\u001b[39m'\u001b[39m: maxls}\n\u001b[0;32m--> 197\u001b[0m res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args\u001b[39m=\u001b[39;49margs, jac\u001b[39m=\u001b[39;49mjac, bounds\u001b[39m=\u001b[39;49mbounds,\n\u001b[1;32m    198\u001b[0m                        \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n\u001b[1;32m    199\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mgrad\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mjac\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    200\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    201\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mfuncalls\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mnfev\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    202\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mnit\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mnit\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    203\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mwarnflag\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[1;32m    204\u001b[0m f \u001b[39m=\u001b[39m res[\u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    360\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m     \u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/model.py:273\u001b[0m, in \u001b[0;36mModel._objective_grads\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_objective_grads\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    272\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_array \u001b[39m=\u001b[39m x\n\u001b[1;32m    274\u001b[0m         obj_f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj_grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective_function(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_gradients(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective_function_gradients())\n\u001b[1;32m    275\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fail_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/parameterized.py:339\u001b[0m, in \u001b[0;36mParameterized.__setattr__\u001b[0;34m(self, name, val)\u001b[0m\n\u001b[1;32m    337\u001b[0m         param \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters[pnames\u001b[39m.\u001b[39mindex(name)]\n\u001b[1;32m    338\u001b[0m         param[:] \u001b[39m=\u001b[39m val; \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__setattr__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name, val)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/core/parameter_core.py:124\u001b[0m, in \u001b[0;36mOptimizationHandlable.optimizer_array\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m#self._highest_parent_.tie.propagate_val()\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_copy_transformed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrigger_update()\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/core/updateable.py:79\u001b[0m, in \u001b[0;36mUpdateable.trigger_update\u001b[0;34m(self, trigger_parent)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_model() \u001b[39mor\u001b[39;00m (\u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_in_init_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_init_):\n\u001b[1;32m     77\u001b[0m     \u001b[39m#print \"Warning: updates are off, updating the model will do nothing\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trigger_params_changed(trigger_parent)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/core/parameter_core.py:134\u001b[0m, in \u001b[0;36mOptimizationHandlable._trigger_params_changed\u001b[0;34m(self, trigger_parent)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mFirst tell all children to update,\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39mthen update yourself.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[39mIf trigger_parent is True, we will tell the parent, otherwise not.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m [p\u001b[39m.\u001b[39m_trigger_params_changed(trigger_parent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m p\u001b[39m.\u001b[39mis_fixed]\n\u001b[0;32m--> 134\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnotify_observers(\u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m trigger_parent \u001b[39melse\u001b[39;49;00m \u001b[39m-\u001b[39;49mnp\u001b[39m.\u001b[39;49minf)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/core/observable.py:91\u001b[0m, in \u001b[0;36mObservable.notify_observers\u001b[0;34m(self, which, min_priority)\u001b[0m\n\u001b[1;32m     89\u001b[0m     which \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m min_priority \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     [callble(\u001b[39mself\u001b[39m, which\u001b[39m=\u001b[39mwhich) \u001b[39mfor\u001b[39;00m _, _, callble \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservers]\n\u001b[1;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m p, _, callble \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservers:\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/core/observable.py:91\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m     which \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m min_priority \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     [callble(\u001b[39mself\u001b[39;49m, which\u001b[39m=\u001b[39;49mwhich) \u001b[39mfor\u001b[39;00m _, _, callble \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservers]\n\u001b[1;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m p, _, callble \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservers:\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/core/parameter_core.py:508\u001b[0m, in \u001b[0;36mParameterizable._parameters_changed_notification\u001b[0;34m(self, me, which)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39mIn parameterizable we just need to make sure, that the next call to optimizer_array\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39mwill update the optimizer_array to the latest parameters\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_copy_transformed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39m# tells the optimizer array to update on next request\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters_changed()\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/core/gp.py:274\u001b[0m, in \u001b[0;36mGP.parameters_changed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparameters_changed\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    266\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m    Method that is called upon any changes to :class:`~GPy.core.parameterization.param.Param` variables within the model.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    In particular in the GP class this method re-performs inference, recalculating the posterior and log marginal likelihood and gradients of the model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39m        this method yourself, there may be unexpected consequences.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposterior, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_marginal_likelihood, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference_method\u001b[39m.\u001b[39;49minference(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkern, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlikelihood, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mY_normalized, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean_function, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mY_metadata)\n\u001b[1;32m    275\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlikelihood\u001b[39m.\u001b[39mupdate_gradients(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_dict[\u001b[39m'\u001b[39m\u001b[39mdL_dthetaL\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    276\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkern\u001b[39m.\u001b[39mupdate_gradients_full(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_dict[\u001b[39m'\u001b[39m\u001b[39mdL_dK\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/inference/latent_function_inference/exact_gaussian_inference.py:53\u001b[0m, in \u001b[0;36mExactGaussianInference.inference\u001b[0;34m(self, kern, X, likelihood, Y, mean_function, Y_metadata, K, variance, Z_tilde)\u001b[0m\n\u001b[1;32m     50\u001b[0m YYT_factor \u001b[39m=\u001b[39m Y\u001b[39m-\u001b[39mm\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m K \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     K \u001b[39m=\u001b[39m kern\u001b[39m.\u001b[39;49mK(X)\n\u001b[1;32m     55\u001b[0m Ky \u001b[39m=\u001b[39m K\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     56\u001b[0m diag\u001b[39m.\u001b[39madd(Ky, variance\u001b[39m+\u001b[39m\u001b[39m1e-8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/kern/src/kernel_slice_operations.py:110\u001b[0m, in \u001b[0;36m_slice_K.<locals>.wrap\u001b[0;34m(self, X, X2, *a, **kw)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mself\u001b[39m, X, X2 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    109\u001b[0m     \u001b[39mwith\u001b[39;00m _Slice_wrap(\u001b[39mself\u001b[39m, X, X2) \u001b[39mas\u001b[39;00m s:\n\u001b[0;32m--> 110\u001b[0m         ret \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, s\u001b[39m.\u001b[39;49mX, s\u001b[39m.\u001b[39;49mX2, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    111\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/caching.py:283\u001b[0m, in \u001b[0;36mCache_this.__call__.<locals>.g\u001b[0;34m(obj, *args, **kw)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     cacher \u001b[39m=\u001b[39m cache[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf] \u001b[39m=\u001b[39m Cacher(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_kwargs, cacher_enabled\u001b[39m=\u001b[39mcache\u001b[39m.\u001b[39mcaching_enabled)\n\u001b[0;32m--> 283\u001b[0m \u001b[39mreturn\u001b[39;00m cacher(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/caching.py:179\u001b[0m, in \u001b[0;36mCacher.__call__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m changed \u001b[39mor\u001b[39;00m not_seen:\n\u001b[1;32m    177\u001b[0m     \u001b[39m# If we need to compute, we compute the operation, but fail gracefully, if the operation has an error:\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         new_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moperation(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/kern/src/stationary.py:114\u001b[0m, in \u001b[0;36mStationary.K\u001b[0;34m(self, X, X2)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m@Cache_this\u001b[39m(limit\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, ignore_args\u001b[39m=\u001b[39m())\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mK\u001b[39m(\u001b[39mself\u001b[39m, X, X2\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    107\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m    Kernel function applied on inputs X and X2.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m    In the stationary case there is an inner function depending on the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    K(X, X2) = K_of_r((X-X2)**2)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scaled_dist(X, X2)\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mK_of_r(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/caching.py:283\u001b[0m, in \u001b[0;36mCache_this.__call__.<locals>.g\u001b[0;34m(obj, *args, **kw)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     cacher \u001b[39m=\u001b[39m cache[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf] \u001b[39m=\u001b[39m Cacher(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_kwargs, cacher_enabled\u001b[39m=\u001b[39mcache\u001b[39m.\u001b[39mcaching_enabled)\n\u001b[0;32m--> 283\u001b[0m \u001b[39mreturn\u001b[39;00m cacher(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/paramz/caching.py:179\u001b[0m, in \u001b[0;36mCacher.__call__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m changed \u001b[39mor\u001b[39;00m not_seen:\n\u001b[1;32m    177\u001b[0m     \u001b[39m# If we need to compute, we compute the operation, but fail gracefully, if the operation has an error:\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         new_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moperation(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/kern/src/stationary.py:168\u001b[0m, in \u001b[0;36mStationary._scaled_dist\u001b[0;34m(self, X, X2)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unscaled_dist(X\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengthscale, X2)\n\u001b[1;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_unscaled_dist(X, X2)\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengthscale\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/kern/src/stationary.py:138\u001b[0m, in \u001b[0;36mStationary._unscaled_dist\u001b[0;34m(self, X, X2)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m X2 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     Xsq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39msquare(X),\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m     r2 \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2.\u001b[39m\u001b[39m*\u001b[39mtdot(X) \u001b[39m+\u001b[39m (Xsq[:,\u001b[39mNone\u001b[39;00m] \u001b[39m+\u001b[39m Xsq[\u001b[39mNone\u001b[39;00m,:])\n\u001b[1;32m    139\u001b[0m     util\u001b[39m.\u001b[39mdiag\u001b[39m.\u001b[39mview(r2)[:,]\u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m \u001b[39m# force diagnoal to be zero: sometime numerically a little negative\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     r2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(r2, \u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39minf)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/util/linalg.py:323\u001b[0m, in \u001b[0;36mtdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtdot\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 323\u001b[0m     \u001b[39mreturn\u001b[39;00m tdot_blas(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/GPy/util/linalg.py:316\u001b[0m, in \u001b[0;36mtdot_blas\u001b[0;34m(mat, out)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m# # Call to DSYRK from BLAS\u001b[39;00m\n\u001b[1;32m    315\u001b[0m mat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masfortranarray(mat)\n\u001b[0;32m--> 316\u001b[0m out \u001b[39m=\u001b[39m blas\u001b[39m.\u001b[39;49mdsyrk(alpha\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, a\u001b[39m=\u001b[39;49mmat, beta\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, c\u001b[39m=\u001b[39;49mout, overwrite_c\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    317\u001b[0m                  trans\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, lower\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    319\u001b[0m symmetrify(out, upper\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    320\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mascontiguousarray(out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for information in function_information:\n",
    "\n",
    "    fun = information['function']\n",
    "    dim = fun.dim\n",
    "    bounds = fun.bounds\n",
    "    standard_bounds=np.array([0.,1.]*dim).reshape(-1,2)\n",
    "    \n",
    "    n_init = 4*dim\n",
    "\n",
    "    \n",
    "    fstar = information['fstar']\n",
    "    \n",
    "    print('fstar is: ',fstar)\n",
    "    \n",
    "    if dim <=3:\n",
    "        step_size = 2\n",
    "        iter_num = 50\n",
    "        N = 10\n",
    "    elif dim<=5:\n",
    "        step_size = 3\n",
    "        iter_num = 100\n",
    "        N = 100\n",
    "    else:\n",
    "        step_size = 3\n",
    "        iter_num = 150\n",
    "        N = 15\n",
    "        \n",
    "    lengthscale_range = [0.001,2]\n",
    "    variance_range = [0.001**2,20]\n",
    "    noise = 1e-6\n",
    "    \n",
    "    print(information['name'])\n",
    "        \n",
    "    \n",
    "    ############################# GP+EI ###################################\n",
    "    BO_KG = []\n",
    "    noise = 1e-6\n",
    "\n",
    "    for exp in range(N):\n",
    "        \n",
    "        print(exp)\n",
    "        \n",
    "        seed = exp\n",
    "\n",
    "        X_BO = get_initial_points(bounds, n_init,device,dtype,seed=seed)\n",
    "        Y_BO = torch.tensor(\n",
    "            [fun(x) for x in X_BO], dtype=dtype, device=device\n",
    "        ).reshape(-1,1)\n",
    "\n",
    "        best_record = [Y_BO.max().item()]\n",
    "        np.random.seed(1234)\n",
    "\n",
    "        for i in range(iter_num):\n",
    "\n",
    "                print(i)\n",
    "                \n",
    "                if i%step_size == 0:\n",
    "                    Y_mean =  Y_BO.mean()\n",
    "                    Y_std = Y_BO.std()\n",
    "            \n",
    "                train_Y = (Y_BO -Y_mean) / Y_std\n",
    "                train_X = normalize(X_BO, bounds)\n",
    "                \n",
    "                minimal = train_Y.min().item()\n",
    "                \n",
    "                train_Y = train_Y.numpy()\n",
    "                train_X = train_X.numpy()\n",
    "                \n",
    "                # train the GP\n",
    "                if i%step_size == 0:\n",
    "                    \n",
    "                    parameters = opt_model_MLE(train_X,train_Y,dim,'GP',noise=noise,seed=i,lengthscale_range=lengthscale_range,variance_range=variance_range)\n",
    "                        \n",
    "                    lengthscale = parameters[0]\n",
    "                    variance = parameters[1]\n",
    "                    \n",
    "                    print('lengthscale: ',lengthscale)\n",
    "                    print('variance: ',variance)\n",
    "                \n",
    "                covar_module =  ScaleKernel(RBFKernel())\n",
    "                train_yvar = torch.tensor(noise, device=device, dtype=dtype)\n",
    "\n",
    "                torch.manual_seed(exp+iter_num)\n",
    "                model = FixedNoiseGP(torch.tensor(train_X), torch.tensor(train_Y), train_yvar.expand_as(torch.tensor(train_Y)),\n",
    "                                     mean_module=ZeroMean(),covar_module=covar_module).to(device)\n",
    "                \n",
    "                model.covar_module.outputscale = variance\n",
    "                covar_module.base_kernel.lengthscale = lengthscale\n",
    "                \n",
    "                model.eval()\n",
    "                \n",
    "                AF = DiscreteKnowledgeGradient(model=model, bounds=torch.tensor(standard_bounds.T),num_discrete_points=100) .to(device)\n",
    "\n",
    "                standard_next_X, _ = optimize_acqf(\n",
    "                    acq_function=AF,\n",
    "                    bounds=torch.tensor(standard_bounds.T) .to(device),\n",
    "                    q=1,\n",
    "                    num_restarts=3*dim,\n",
    "                    raw_samples=100,\n",
    "                    options={},\n",
    "                )\n",
    "\n",
    "                \n",
    "                # EI = ExpectedImprovement(model=model, best_f=np.max(train_Y)) .to(device)\n",
    "\n",
    "                # standard_next_X, _ = optimize_acqf(\n",
    "                #     acq_function=EI,\n",
    "                #     bounds=torch.tensor(standard_bounds.T) .to(device),\n",
    "                #     q=1,\n",
    "                #     num_restarts=3*dim,\n",
    "                #     raw_samples=30*dim,\n",
    "                #     options={},\n",
    "                # )\n",
    "                \n",
    "                \n",
    "                \n",
    "                # kernel = GPy.kern.RBF(input_dim=dim,lengthscale=lengthscale,variance=variance)\n",
    "                # m = GPy.models.GPRegression(train_X.reshape(-1,dim), train_Y.reshape(-1,1),kernel)\n",
    "                # m.Gaussian_noise.fix(noise)\n",
    "\n",
    "                # np.random.seed(i)\n",
    "                # standard_next_X = EI_acquisition_opt(m,bounds=standard_bounds,f_best=minimal)\n",
    "                \n",
    "                \n",
    "                X_next = unnormalize(torch.tensor(standard_next_X), bounds).reshape(-1,dim)            \n",
    "                Y_next = fun(X_next).reshape(-1,1)\n",
    "\n",
    "                # Append data\n",
    "                X_BO = torch.cat((X_BO, X_next), dim=0)\n",
    "                Y_BO = torch.cat((Y_BO, Y_next), dim=0)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # final choice is EI\n",
    "                kernel = GPy.kern.RBF(input_dim=dim,lengthscale=lengthscale,variance=variance)\n",
    "                m = GPy.models.GPRegression(train_X.reshape(-1,dim), train_Y.reshape(-1,1),kernel)\n",
    "                m.Gaussian_noise.fix(noise)\n",
    "\n",
    "                np.random.seed(i)\n",
    "                standard_next_X_final = EI_acquisition_opt(m,bounds=standard_bounds,f_best=minimal)\n",
    "                \n",
    "                X_next_final = unnormalize(torch.tensor(standard_next_X_final), bounds).reshape(-1,dim)            \n",
    "                Y_next_final = fun(X_next_final).reshape(-1,1)\n",
    "\n",
    "                # Append data\n",
    "                X_BO_final = torch.cat((X_BO, X_next_final), dim=0)\n",
    "                Y_BO_final = torch.cat((Y_BO, Y_next_final), dim=0)\n",
    "                \n",
    "                best_record.append(Y_BO_final.max().item())\n",
    "                \n",
    "                print(best_record[-1])\n",
    "                \n",
    "                noise = variance*10**(-5)   #adaptive noise\n",
    "                noise = np.round(noise, -int(np.floor(np.log10(noise))))\n",
    "                print('noise: ',noise)\n",
    "                \n",
    "        best_record = np.array(best_record) \n",
    "        BO_KG.append(best_record)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_bounds.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "known_boundary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
